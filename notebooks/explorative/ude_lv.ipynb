{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically Discover Missing Physics by Embedding Machine Learning into Differential Equations\n",
    "\n",
    "https://docs.sciml.ai/Overview/dev/showcase/missing_physics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DrWatson setup\n",
    "using DrWatson\n",
    "@quickactivate \"diff_gleam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciML Tools\n",
    "using OrdinaryDiffEq, ModelingToolkit, SciMLSensitivity#,DataDrivenDiffEq, DataDrivenSparse\n",
    "using Optimization, OptimizationOptimJL #, OptimizationOptimisers\n",
    "\n",
    "# Standard Libraries\n",
    "using LinearAlgebra, Statistics\n",
    "\n",
    "# External Libraries\n",
    "using ComponentArrays, Lux, Zygote, Plots, StableRNGs\n",
    "gr()\n",
    "\n",
    "# Set a random seed for reproducible behaviour\n",
    "rng = StableRNG(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.add(\"SciMLSensitivity\")\n",
    "# Pkg.add(\"Zygote\")\n",
    "# Pkg.add(\"StableRNGs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "Lotka-Volterra equations\n",
    "\n",
    "## Generate training data\n",
    "Using the full equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka!(du,u,p,t) #general form for ODE solvers\n",
    "    α, β, γ, δ = p\n",
    "    du[1] = α * u[1] - β * u[2] * u[1]\n",
    "    du[2] = γ * u[1] * u[2] - δ * u[2]\n",
    "end\n",
    "\n",
    "# Experimental parameters\n",
    "tspan = (0.0, 5.0)\n",
    "u0 = 5.0f0 * rand(rng, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ = [1.3, 0.9, 0.9, 1.8]\n",
    "prob = ODEProblem(lotka!, u0, tspan, p_)\n",
    "solution = solve(prob, Vern7(), abstol = 1e-12, reltol = 1e-12, saveat = 0.25)\n",
    "X = Array(solution)\n",
    "t = solution.t\n",
    "\n",
    "x_bar = mean(X, dims = 2)\n",
    "noise_magniutde = 5e-3\n",
    "Xₙ = X .+ (noise_magniutde * x_bar) .* randn(rng, eltype(X), size(X)) #eltype(X) = float64\n",
    "\n",
    "plot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing])\n",
    "scatter!(t, transpose(Xₙ), color = :red, label = [\"Noisy Data\" nothing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.sciml.ai/DiffEqDocs/stable/solvers/ode_solve/: Vern7 =  Verner's “Most Efficient” 7/6 Runge-Kutta method. (lazy 7th order interpolant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the UDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf(x) = exp.(-(x .^ 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Radial_basis_function\n",
    "\n",
    "Note (ChatGPT generated): \n",
    "In Julia, the const keyword is used to declare constants. Constants are variables whose values cannot be changed after they are assigned a value. const is often used to declare global constants that are known at compile time and should not be modified during the execution of the program. Here are some common use cases for const in Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const U = Lux.Chain(\n",
    "    Lux.Dense(2, 5, rbf), Lux.Dense(5, 5, tanh), Lux.Dense(5, 5, rbf),\n",
    "    Lux.Dense(5,2)\n",
    ")\n",
    "p_nn, st = Lux.setup(rng, U)\n",
    "const _st = st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U([1,1], p_nn, _st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ude_dynamics!(du, u, p, t, p_true)\n",
    "    u_hat = U(u, p, _st)[1]\n",
    "    du[1] = p_true[1] * u[1] + u_hat[1] #p_true[1] = alpha\n",
    "    du[2] = -p_true[4] * u[2] + u_hat[2] #p_true[4] = gamma\n",
    "end \n",
    "#insert known parameters\n",
    "nn_dynamics!(du, u, p, t) = ude_dynamics!(du, u, p, t, p_)\n",
    "#Define the problem\n",
    "prob_nn = ODEProblem(nn_dynamics!,Xₙ[:,1], tspan, p_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEY: the parameter vector of the ODE are the WEIGHTS of the NN!\n",
    "\n",
    "`remake` for modifying an ODEProblem: https://docs.sciml.ai/DiffEqDocs/stable/basics/problem/#Modification-of-problem-types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(θ, X = Xₙ[:,1],T = t)\n",
    "    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n",
    "    Array(solve(\n",
    "        _prob, Vern7(), saveat = T, \n",
    "        abstol = 1e-6, reltol = 1e-6,\n",
    "        sensealg = QuadratureAdjoint(autojacvec = ReverseDiffVJP(true))\n",
    "    ))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.sciml.ai/SciMLSensitivity/stable/manual/differential_equation_sensitivities/#Choosing-a-Sensitivity-Algorithm on chose of calculating derivatives!\n",
    "\n",
    "Comparison to the `diffeqflux.ipynb` notebook: there we do NOT make this prediction function ourselves, instead we use the built-in forward mode prediction of the `NeuralODE` interface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(θ)\n",
    "    X_hat = predict(θ)\n",
    "    sum(abs2, Xₙ .- X_hat)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xₙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(p_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xₙ .- predict(p_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs2.(Xₙ .- predict(p_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(t, transpose(predict(p_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(p_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Lots of parameters -> use reverse mode AD with Zygote\n",
    "Use https://docs.sciml.ai/Optimization/stable/API/optimization_function/ from Optim.jl.\n",
    "Idea here is that optimisation function is of form:\n",
    "$$\n",
    "\\min_u f(u,p)\n",
    "$$\n",
    "with generally speaking $u$ the state variables and $p$ the parameters. So in the case of classic optimisation like the Rosenbrock equation (https://en.wikipedia.org/wiki/Rosenbrock_function) $u$ is $x,y$ while $p$ are parameters of the function, so the goal is to find the optimum values of $x$ and $y$.\n",
    "For our case, we want to find the optimal values of the NN weights, so we should treat these weights as the states ($u$) of our optimisation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adtype = Optimization.AutoZygote()\n",
    "optf = Optimization.OptimizationFunction((x,p) -> loss(x), adtype) \n",
    "optprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(typeof(p_nn))\n",
    "println(typeof(ComponentVector{Float64}(p_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ComponentVector part of the `ComponentArrays.jl` package (https://jonniedie.github.io/ComponentArrays.jl/stable/quickstart/)\n",
    "Optimisation strategy:\n",
    "- First use ADAM for finiding a general area of the parameter space\n",
    "- Then use BFGS for honing in on local minimum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create callback function\n",
    "losses = Float64[] #initialie an emtpy array!\n",
    "\n",
    "callback = function(p, loss)\n",
    "    push!(losses, loss)\n",
    "    if length(losses) % 50 == 0\n",
    "        println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "    end\n",
    "    return false #obliged to specify that optimization should NOT be halted!\n",
    "end\n",
    "\n",
    "using OptimizationFlux\n",
    "ADAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = Optimization.solve(optprob, ADAM(), callback = callback, maxiters = 5000)\n",
    "println(\"Training loss after $(length(losses)) iterations: $(losses[end])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optprob2 = Optimization.OptimizationProblem(optf, res1.u) #res1.u is the new u0!\n",
    "using OptimizationOptimJL\n",
    "res2 = Optimization.solve(optprob2, LBFGS(), callback = callback, maxiters = 3000)\n",
    "println(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "\n",
    "# Rename the best candidate\n",
    "p_trained = res2.u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_losses = plot(1:5000, losses[1:5000], yaxis = :log10, xaxis = :log10, label = \"ADAM\",\n",
    "xlabel = \"Iterations\", ylabel = \"Loss\")\n",
    "plot!(5001:length(losses), losses[5001:end], label = \"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = first(solution.t):mean(diff(solution.t))/2:last(solution.t)\n",
    "X_hat = predict(p_trained, Xₙ[:,1], ts)\n",
    "\n",
    "plot_hybrtid = plot(ts, transpose(X_hat), color = [:blue :green], label = [\"UDE Approximation u1\" \"UDE Approximation u2\"])\n",
    "scatter!(solution.t, transpose(Xₙ) , color = [:blue :green], label = [\"Noisy measurements u1\" \"Noisy measurements u2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice fit now :)\n",
    "\n",
    "Compare the learned interactions with what was fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideal unknown interactions of the predictor\n",
    "Ȳ = [-p_[2] * (X_hat[1, :] .* X_hat[2, :])'; p_[3] * (X_hat[1, :] .* X_hat[2, :])']\n",
    "# Neural network guess\n",
    "Ŷ = U(X_hat, p_trained, st)[1]\n",
    "\n",
    "plot(ts, transpose(Ȳ), color = [:blue :green], label = [\"real dynamics u1\" \"real dynamics u2\"])\n",
    "plot!(ts, transpose(Ŷ), color = [:blue :green], linestyle = :dashdot, label = [\"estimated dynamics u1\" \"estimated dynamics u2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic regression via sparse regression (SINDy based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
